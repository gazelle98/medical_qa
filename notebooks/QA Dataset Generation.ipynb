{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cufflinks as cf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from urllib.parse import unquote\n",
    "from wordcloud_fa import WordCloudFa\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data from Wikipedia Manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding a question and an answer from an article in Wikipedia, assign each to their relevant variable.\n",
    "Pay attention to do not run it again for a single set of values since it duplicates the dataset's rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['question', 'answer', 'link', 'paragraph_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'درمان نشانگان هلپ چگونه انجام می‌شود؟'\n",
    "answer = '''فعلا، تنها و موثرترین درمان توصیه شده، زایمان است، زیرا باعث بهبود علایم می شود و با خروج جفت به تدریج علائم کاملا از بین می روند. زایمان سریع تنها گزینه قابل قبول در مواردی است که منجر به نارسایی چند ارگان شده، خونریزی رخ داده و یا خطر قابل توجه برای جنین وجود دارد.\n",
    "\n",
    "بعضی از داروها نیز برای درمان علائم خاص استفاده می شوند.'''\n",
    "link = 'https://fa.wikipedia.org/wiki/%D9%86%D8%B4%D8%A7%D9%86%DA%AF%D8%A7%D9%86_%D9%87%D9%84%D9%BE'\n",
    "paragraph_text = '''فعلا، تنها و موثرترین درمان توصیه شده، زایمان است، زیرا باعث بهبود علایم می شود و با خروج جفت به تدریج علائم کاملا از بین می روند. زایمان سریع تنها گزینه قابل قبول در مواردی است که منجر به نارسایی چند ارگان شده، خونریزی رخ داده و یا خطر قابل توجه برای جنین وجود دارد.\n",
    "\n",
    "بعضی از داروها نیز برای درمان علائم خاص استفاده می شوند.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append({\n",
    "    'question': question, \n",
    "    'answer': answer,\n",
    "    'link': link,\n",
    "    'paragraph_text': paragraph_text}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/1000.csv')  # saving data to a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Crawled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/1000.csv', index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='question').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Wikipedia's Pages Content, Categories and Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to crawl categories, title and whole content of the relevant article for a row in the dataset, you should run the cell below. It will take a while to run so be aware of running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang('fa')\n",
    "wikipedia.set_rate_limiting(rate_limit=True)\n",
    "\n",
    "df['categories'] = np.nan\n",
    "df['categories'] = df['categories'].astype('object')\n",
    "\n",
    "for i, v in df.iterrows(): \n",
    "    topic = df.iloc[i].link.replace('https://fa.wikipedia.org/wiki/', '')\n",
    "    topic = unquote(topic)\n",
    "    a = wikipedia.WikipediaPage(topic)\n",
    "    df.loc[i, 'text'] = a.content\n",
    "    df.loc[i, 'title'] = a.title\n",
    "    df.at[i, 'categories'] = a.categories\n",
    "    print('yay ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/1000.csv')  # saving the expanded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Numbers, Punctuations and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = open('../data/raw/stopwords.txt', 'r')\n",
    "stopwords = list(map(lambda x: x.strip(), stopwords_file.readlines())) + ['ویکی', 'پدیا', 'جستارهای', 'وابسته']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punction_marks_and_numbers(word):\n",
    "    punc = ['!','\"','#','(',')','*',',','-','.','/',':','[',']','«','»','،','؛', '؟', '\\n']\n",
    "    nums = ['۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹', '۰']\n",
    "    whole = punc + nums + list(string.printable)\n",
    "    \n",
    "    for i in whole:\n",
    "        word = word.replace(i, ' ')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_odds(sentence):\n",
    "    words = sentence.split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        word = word.replace('\\u200c', ' ')\n",
    "        word = word.replace('\\u200e', ' ')\n",
    "        result.append(replace_punction_marks_and_numbers(word))\n",
    "    return ' '.join(' '.join(result).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = df['answer'].apply(lambda x: remove_odds(x))\n",
    "df['question'] = df['question'].apply(lambda x: remove_odds(x))\n",
    "df['text'] = df['text'].apply(lambda x: remove_odds(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Simple Plots for Answers and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "df['answer_len'] = df['answer'].apply(len)\n",
    "df['answer_len'].iplot(\n",
    "    kind='hist',\n",
    "    bins=100,\n",
    "    xTitle='answer length',\n",
    "    linecolor='black',\n",
    "    color='red',\n",
    "    yTitle='count',\n",
    "    title='Answer Length Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question_len'] = df['question'].apply(len)\n",
    "df['question_len'].iplot(\n",
    "    kind='hist',\n",
    "    bins=100,\n",
    "    xTitle='quesiton length',\n",
    "    linecolor='black',\n",
    "    color='blue',\n",
    "    yTitle='count',\n",
    "    title='Question Length Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing Word Cloud for articles' content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_common_words(df, column, n=20):\n",
    "    words = []\n",
    "    rows = list(df[column].values)\n",
    "    for i in rows:\n",
    "        words.extend(i.split())\n",
    "    return Counter(words).most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_temp_df = pd.DataFrame(get_n_common_words(df, 'answer'), columns = ['word', 'count'])\n",
    "q_temp_df = pd.DataFrame(get_n_common_words(df, 'question'), columns = ['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(df.text.values)\n",
    "text2 = []\n",
    "for t in text.split():\n",
    "    if t not in stopwords:\n",
    "        text2.append(t)\n",
    "text2 = ' '.join(text2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloudFa(width=1000, height=500, background_color='white')\n",
    "wc = wordcloud.generate(text2)\n",
    "image = wc.to_image()\n",
    "image.show()\n",
    "image.save('../data/visualized/wordcloud.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
